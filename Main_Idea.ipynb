{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c21ca0",
   "metadata": {},
   "source": [
    "  <style>\n",
    "    .body{\n",
    "          backgroung-color:white; color:red;font-size:30px;\n",
    "    }\n",
    "    \n",
    "    </style>\n",
    "  <div class=\"body\">\n",
    "  <center>\n",
    "  <strong > Main Idea of Attention in KGAT </strong>\n",
    "  </center>\n",
    "  </div>\n",
    "\n",
    "\n",
    "\n",
    "KGAT uses **relation-aware attention** to weigh the importance of neighbors (entities) in the KG. Unlike static aggregation methods, attention allows the model to:\n",
    "- Learn which neighbors (e.g., items, genres, users) are most relevant to a target node (e.g., a user or item).\n",
    "- Account for **relation types** (e.g., \"likes,\" \"same genre\") when assigning weights, capturing how different interactions influence recommendations.\n",
    "\n",
    "\n",
    "\n",
    "### **1. How Attention Works in KGAT**\n",
    "#### **a. Relation-Specific Transformations**\n",
    "Each relation type $ r $ (e.g., \"user-item interaction,\" \"item-genre\") has a **learnable weight matrix** $ W_r $. When computing attention between a node $ h $ (head entity) and its neighbor $ t $ (tail entity) via relation $ r $:\n",
    "- Both entities are transformed using $ W_r $:  \n",
    "  $$\n",
    "  \\hat{h}_r = W_r h, \\quad \\hat{t}_r = W_r t\n",
    "  $$\n",
    "- This ensures the attention mechanism considers the **semantic meaning** of the relation.\n",
    "\n",
    "#### **b. Attention Coefficients**\n",
    "Attention scores are computed using a **LeakyReLU** neural network over concatenated transformed vectors:\n",
    "$$\n",
    "\\alpha_{h,t} = \\text{LeakyReLU}\\left(a^T [\\hat{h}_r \\parallel \\hat{t}_r]\\right)\n",
    "$$\n",
    "- $ a $ is a trainable attention vector.\n",
    "- The scores are normalized via **softmax** across all neighbors of the target node:  \n",
    "  $$\n",
    "  \\alpha_{h,t} = \\text{softmax}(\\alpha_{h,t})\n",
    "  $$\n",
    "- This ensures that the sum of weights for all neighbors equals 1.\n",
    "\n",
    "#### **c. Message Passing**\n",
    "The target node aggregates messages from its neighbors using the attention coefficients:\n",
    "$$\n",
    "v_i^{(l+1)} = \\sigma\\left(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{i,j} W_r v_j^{(l)}\\right)\n",
    "$$\n",
    "- $ v_i^{(l+1)} $: Updated embedding for node $ i $ at layer $ l+1 $.\n",
    "- $ \\mathcal{N}_i $: Neighbors of node $ i $.\n",
    "- $ \\sigma $: Activation function (e.g., ReLU).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **2. Key Features of KGAT’s Attention Mechanism**\n",
    "#### **a. Heterogeneous Graph Handling**\n",
    "- Nodes (users, items, entities) and edges (relations) are **heterogeneous**.\n",
    "- Attention scores depend on **both node features and relation types**, allowing the model to:\n",
    "  - Differentiate between weak and strong signals (e.g., \"clicked\" vs. \"purchased\").\n",
    "  - Prioritize relations critical for recommendations (e.g., \"same genre\" for movie suggestions).\n",
    "\n",
    "#### **b. Multi-Head Attention**\n",
    "- **Multiple attention heads** are used to stabilize training and capture diverse patterns.\n",
    "- Final embeddings are concatenated or averaged across heads:  \n",
    "  $$\n",
    "  v_i^{(l+1)} = \\parallel_{k=1}^K \\sigma\\left(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{i,j}^{(k)} W_r^{(k)} v_j^{(l)}\\right)\n",
    "  $$\n",
    "  - $ K $: Number of attention heads.\n",
    "\n",
    "#### **c. End-to-End Training**\n",
    "- Attention coefficients are optimized via gradients from the **recommendation loss** (e.g., BPR loss for implicit feedback).\n",
    "- The model learns to focus on neighbors that improve prediction accuracy (e.g., users with similar preferences or items with complementary attributes).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **3. Example: User-Item Recommendations**\n",
    "Consider a user $ u $ connected to items $ i_1, i_2, i_3 $ via \"interacted_with\" and items $ i_2, i_4 $ via \"purchased\":\n",
    "1. **Relation-Specific Attention**: The model assigns higher weights to \"purchased\" items ($ i_2, i_4 $) than \"interacted_with\" items ($ i_1, i_3 $) because purchases are stronger signals.\n",
    "2. **Neighbor Importance**: If $ i_2 $ is a popular item similar to items $ u $ previously liked, its attention weight increases, influencing $ u $’s embedding.\n",
    "3. **Propagation Layers**: Over multiple layers, information propagates further (e.g., from items to their genres, then to related items), refining user/item representations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **4. Benefits of Attention in KGAT**\n",
    "- **Personalization**: Focuses on neighbors unique to individual user preferences.\n",
    "- **Interpretability**: Attention scores highlight why a recommendation was made (e.g., \"recommended *The Matrix* because you liked *Inception* and both are directed by Christopher Nolan\").\n",
    "- **Scalability**: Attention reduces noise by ignoring irrelevant entities in large KGs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **5. Challenges**\n",
    "- **Computational Complexity**: Softmax over many neighbors can be expensive, though sampling strategies mitigate this.\n",
    "- **Cold Start**: Attention may struggle for new users/items with few connections until more data is available.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
